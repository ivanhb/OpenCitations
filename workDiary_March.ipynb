{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Thursday 1/3/2018:\n",
    "Two important additions to the 'cociprocess' script:\n",
    "* A new heuristic to handle the negative time span: in case the timespan calculated with the cited document is negative, we calculate its timespan with the date in the ‘year’ field, and in case this one is positive we take this value instead.\n",
    "* Update the dates dictionary in case we find a much detailed date value for the same id\n",
    "\n",
    "Other issues have been fixed in the script that will execute in parallel cociprocess.py. Most importantly, the script will keep track of the processes in execution and re-run a process in case it stops for some reason.  \n",
    "I have executed the script on our server for the ‘open/’ data stored. I will check the results of it tomorrow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Friday 2/3/2018:\n",
    "I had run the script father that creates N number of subprocesses which processes the crossref dumb and create the resulting CSV files. N equals the number of inner folders in the root directory path given to the father script. Here is the actual command (executed from the ‘script/’ folder):\n",
    "\n",
    "```nohup python3.5 multi_cociprocess.py -script cociprocess.py -in /srv/data/coci/open/ -out /srv/data/coci/results/open/ -lookup /home/ivanhb/lookup.csv &```\n",
    "\n",
    "The first output raised some small issues, which I have fixed with additional controls added to the script. \n",
    "Despite this there is a much more concerning problem: Although the processes have been all created they stay too much time in sleeping status. The processes don’t run continuously, this behavior slows down a lot the expected performances.  \n",
    "I have tried several tests, and modified several parameters of the Popen python command, but still without positive feedbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Monday 5/3/2018:\n",
    "We are still stuck with a processes parallelism problem. Both me and Silvio are looking for techniques and possible solutions to this problem. Since we still cannot let all the processes stay active, their scheduling paradigm slows a lot the entire dump elaboration. I spent the day searching the web for some approaches and strategies that might change this situation. \n",
    "* Modifying the multi_cociprocess.py script by using the multiprocessing library. \n",
    "* Applying different parameters to the Popen call that could affect its execution.\n",
    "* Monitoring the outputs of the scripts, in case unexpected behaviors happened  \n",
    "* Creating multiple cociprocess.py scripts to execute for each different process\n",
    "\n",
    "Unfortunately still without positive feedbacks from the execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Tuesday 6/3/2018:  \n",
    "After a quick review of the situation and the processes parallelism problem, together with Silvio we decided to give another last chance, and try one last approach that might help us solve our problem.  \n",
    "We have decided to use [Parallel Gnu](https://www.gnu.org/software/parallel/) and create a small script that generates a file containing all the commands that need to be executed (a cociprocess.py call for each folder of the crossref dump). The script output will be used as an input to Parallel Gnu.  \n",
    "Even this last strategy, and after the application of different parameters to parallel, didn't give us positive results.  \n",
    "Therefore we decided to move on, and elaborate an alternative strategy to process our crossref dump. The main idea is to limit as much as possible the GET requests (we will use only direct DOI requests).  \n",
    "I have created a first python script that generates our new global index containing the publication date of each DOI, along with the updated lookup codes list. The second Script needed is the one that will generate all the data and provenance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Wednesday 14/3/2018:\n",
    "Today with Silvio we have discussed the overall current situation regarding the coci project and the SAVE-SD adjustments. \n",
    "* SAVE-SD: I have listed all the comments made by the reviewers and classified them in points that we agree/not-agree we need to change. And discussed with Silvio all the other doubtful aspects.\n",
    "* Coci processing: There was a corrupted file in the limited dataset and we have been able to successfully correct it. I have launched the global index generator on the new limited correct dataset. The script have terminated successfully and the total number of entries we have in the dates csv file is equal to: 7759816.\n",
    "\n",
    "I will now first concentrate on the SAVE-SD paper modifications, before moving back to the coci project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Thursday 15/3/2018:\n",
    "I have started and almost finished modifying the SAVE-SD paper of OSCAR. I am planing on working on it tomorrow morning also and send it to Silvio (the .odt draft document).   \n",
    "All the modifications made have been discussed with Silvio yesterday.   \n",
    "Tomorrow, I will give a brief look on the adjustments made today and just add 2 screenshots to show the scholarydata and Wikidata examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Friday 16/3/2018:\n",
    "I have completed the last modifications in the OSCAR draft and send it to Silvio. The last things I have done on it are the integrations of new figures and modify the diagram of OSCARs workflow.  \n",
    "Next I moved back on COCI. The global indexes generated (also for the limited dataset) look correct.  \n",
    "So I tested the second script, which will process all the references and generate all the data, as it was in our first attempt to process COCI. After some adaptions the results obtained locally are those expected. I have lunched it also on the server for the ‘limited’ directory, by typing the following command from the script directory: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "``` nohup python multi_cociprocess_refs.py -pycmd python -script cociprocess_refs.py -in /srv/data/coci/limited/ -glob /srv/data/coci/results/plan-b/limited/ -out /srv/data/coci/results/plan-b/limited/ &```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Monday 19/3/2018:\n",
    "At this point we need to prepare the RDF generation phase for the COCI csv data produced. I have started first by modifying a bit the [citation.py](https://github.com/opencitations/coci/blob/master/script/citation.py) module written by Silvio to handle the COCI case:    \n",
    "The citation object can already be initialized given the Duration and Publication-date as value, this will prevent the citation module from calculating it. The default values are None in case we don’t have these values at the initialization.  \n",
    "In case the oci is not specified let the object init create it, otherwise (coci case) we already have it, and therefor we can specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self,\n",
    "                 ...\n",
    "             ,creation_date = None, duration= None, oci= None):\n",
    "    \n",
    "    \n",
    "    if oci == None:\n",
    "            self.oci = citing_entity_local_id + \"-\" + cited_entity_local_id        \n",
    "    \n",
    "    ...\n",
    "    \n",
    "    self.creation_date = creation_date\n",
    "    self.duration = duration\n",
    "    if self.creation_date == None and self.duration == None :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next I moved on writing [coci_rdfgen.py](https://github.com/opencitations/coci/blob/master/script/coci_rdfgen.py) a python script which will open the COCI csv data files and call the citation.py to generate the RDF entities and generate the corresponding files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
